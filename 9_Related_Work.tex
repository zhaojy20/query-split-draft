Our work is relevant to three research directions: (a) the literatures that studies the intrinsic difficulty of accurate cardinality estimation; (b) adaptive query processing \cite{paper57}; (c) researches that aim to improve accuracy of cardinality estimation. We state existing works in these directions respectively in Section \ref{S81}, \ref{S82}, and \ref{S83}.
\subsection{Difficulty of Cardinality Estimation} \label{S81}
    On theoretical front, Yannis and Stavros \cite{paper31} proved that cardinality estimation errors would grow exponentially with join size. They study the natural join and assume there is an error on the estimated distribution of join attributes in each relation.\par
    Leis et al. \cite{JOB} pay attention to practice and use experiments to show that cardinality estimators in commercial database produce large errors in real workload. They introduced a new benchmark called ``Join Order Benchmark", which is simple but challenging. Their experiments suggest that although cardinality estimator is accurate for one relation queries, no existing techniques can accurately estimate cardinality with high correlated join predicates.\par
    Their research shows the necessity for new query optimization frameworks and the intrinsic difficulty of accurate cardinality estimation, which inspires our research on dynamic query optimization.

\subsection{Adaptive Query Processing} \label{S82}
    According to reference \cite{paper47}, there are three families in adaptive query processing: plan-based system (re-optimization), routing-based system and continuous-query-based system.  
    
    \subsubsection{Re-optimization} The most relevant work in the literature is re-optimization. Re-optimization monitors the execution of the current plan and re-optimizes whenever the actual condition differs significantly from the estimations made by the optimizer.\par
    \textit{Reopt} \cite{Reopt} and \textit{Pop} \cite{Pop} are the two most representative works on re-optimization. \textit{Reopt} adds statistic collection operators to the execution plan and collects statistics after a segment has been executed. When finding that the collected statistic deviates too much from estimation, the database triggers re-optimization and re-optimizes the unexecuted part by the collected statistic.\par
    The idea of \textit{Pop} is the same as \textit{Reopt}, but \textit{Pop} can trigger re-optimization in more join nodes, like the outer side of nest-loop join, while \textit{Reopt} can only trigger re-optimization at the blocked operator.\par
    Conquest \cite{paper39} extends the re-optimization to a parallel execution environment. Query incremental execution \cite{QIE} estimates the error of cardinality estimation at each operator and materializes the operator that has the largest error currently. To decrease the time of collecting the intermediate statistics, sampling re-optimization \cite{SampleReopt} throws the plan to sampling-based estimator. In recent work, Cuttlefish \cite{paper29} uses reinforcement learning techniques to explore possible physical operators during query execution and exploits the fastest ones. However, Cuttlefish can only modify the physical operator, but is unable to change the join order.\par
    Recently, Perron et al. \cite{MichaelStonebraker} simulates re-optimization in PostgreSQL. They first observe the error by the ``explain" command and materialize the intermediate result that deviates too much from estimation as a temporary table. Then they rewrite the query by replacing the materialized part with the temporary table, and finally they call the optimizer again. Their result shows that re-optimization in PostgreSQL can sharply improve the execution time.\par
\textcolor{blue}{
    Compared to the \textit{query split} framework, the query execution of the above re-optimizations is still based on the global plan. And if the global plan is terrible, re-optimization's sub-plan can also be terrible. Note that the first time of materialization in re-optimization is before the first re-optimization is triggered. Hence, if the sub-plan we choose initially is a sub-optimal plan, we lose the opportunity to correct this wrong decision.
    }
    
    \subsubsection{Routing-based system} Routing-based systems eliminate the traditional optimizer and query plans. They process queries by routing tuples through a pool of operators. The idea of the routing-based system can be traced back to INGRES \cite{paper46}. In INGRES, each tuple could be executed in a different order. The routing-based system eliminates the concept of execution plan by routing tuples one by one. Eddies \cite{paper30} adds a new operator called ripple join, and Eddies can change the join order in ripple join.\par
    Both re-optimization and routing-based system break the unidirectional pipeline between optimizer and executor. However, compared to re-optimziation, routing-based systems totally abandon optimizer, making routing algorithms highly dependent on greedy algorithm and therefore unsuitable for complex queries \cite{paper47}.
    
    \subsubsection{Continuous-query-based System} Continuous-Query-based, or CQ-based, systems are used for queries that will run many times or a very long time, which is prevalent in the data stream systems.\par
    Although CQ-based systems also support variable execution plan during execution, however, there is a huge gap between CQ-base systems and our work. CQ-base systems pay attention to the run-time change of stream characteristics and system conditions, rather than cardinality estimation error for a given query.
    
    \subsubsection{Others} Dynamic query plan \cite{paper25, paper42, paper43} and parametric query optimization \cite{paper44} are proposed to change the query plan without calling the optimizer again. They give a set of possible optimal plans to the executor instead of giving one single optimal plan and during execution, the plans can switch to each other.\par
    Similar to other adaptive query processing techniques, these works also pay attention to a flexible execution plan. But compared to our work, query optimization and execution in dynamic query plan and parametric query optimization is not interleaved, which leads to a great overhead to maintain multiple execution plans.

\subsection{Cardinality Estimation} \label{S83}
    Cardinality estimation techniques \cite{paper33} are also relevant. The researches in cardinality estimation techniques aim to improve cardinality estimation. There are two main research areas: data-driven cardinality estimator \cite{paper1, paper5, paper8, paper9, paper17, paper19, paper21, paper22, paper23, paper36}, query-driven cardinality estimator \cite{paper12, paper14, paper15, paper16, paper18, paper34, paper37}. The data-driven cardinality estimator approximates the data distribution of a table by mapping each tuple to its probability of occurrence in the table. The query-driven cardinality estimator uses some models to learn the mapping between queries and cardinalities.\par
    We emphasize that there is no conflict between dynamic query optimization and advanced cardinality estimation techniques. On the contrary, \textit{query split} benefits from the improvement of cardinality estimation on small join. Making an optimal plan in each sub-query can significantly enhance overall performance.\par
\textcolor{blue}{
    One related work is USE \cite{USE} because we both execute the query by sub-query form and use the same idea that uses Pk-Fk join in constraining the sub-queries size. USE pushed the non-expand operators (i.e., filter and Pk-Fk join) to the sub-query, similar to our \textit{RelationshipCenter}. It then used sketch-based cardinality estimation to decide the join order between sub-queries. However, USE is not an adaptive query processing method, and sketch-based cardinality estimation needs pre-processing of the dataset.
}